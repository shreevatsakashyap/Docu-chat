{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Replace with actual import for the library you're using\n",
    "\n",
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents\n",
    "\n",
    "def load_documents_from_directories(directories):\n",
    "    all_documents = []\n",
    "    for directory in directories:\n",
    "        documents = read_doc(directory)\n",
    "        all_documents.extend(documents)\n",
    "    return all_documents\n",
    "\n",
    "# Example usage\n",
    "directories = [\n",
    "    \"C:/Users/samar/Desktop/Langchat/data/Langchain/Components/pdf_files\",\n",
    "    \"C:/Users/samar/Desktop/Langchat/data/Langchain/LangChain Expression Language/pdf_files\",\n",
    "    \"C:/Users/samar/Desktop/Langchat/data/Langchain/Use cases/pdf_files\",\n",
    "    \"C:/Users/samar/Desktop/Langchat/data/Langsmith/Pdf_files\",\n",
    "    \"C:/Users/samar/Desktop/Langchat/data/Langchain/Rag\"\n",
    "\n",
    "]\n",
    "\n",
    "all_documents = load_documents_from_directories(directories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1900"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(docs,chunk_size=800,chunk_overlap=20):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    doc= text_splitter.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = chunk_data(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5603"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\samar\\Desktop\\Langchat\\lang\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://838581f9-41df-47dc-a452-47ea8f1e1edc.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
    "api_key = \"7QBIL3BHx33BOqbHlsSIrn5in7kTX4OLzn5FAXQYPFP7Y1KFYbagpg\"\n",
    "qdrant1 = Qdrant.from_documents(\n",
    "    doc,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=True,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"langchain\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is recursie text splitter\"\n",
    "found_docs = qdrant1.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='RAGOn this pageQ&A with RAGOverview?One of the most powerful applications enabled by LLMs\\nis sophisticated question-answering (Q&A) chatbots. These are applications that can answer\\nquestions about specific source information. These applications use a technique known as Retrieval\\nAugmented Generation, or RAG.What is RAG??RAG is a technique for augmenting LLM knowledge', metadata={'page': 0, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langchain\\\\Use cases\\\\pdf_files\\\\3.pdf', '_id': '1c8fdbae-054f-434d-a3a0-77b17224b1af', '_collection_name': 'langchain'}),\n",
       " Document(page_content='step.LangChain provides all the building blocks for RAG applications - from simple to complex.\\nThis section of the documentation covers everything related to the retrieval step - e.g. the fetching of', metadata={'page': 0, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langchain\\\\Rag\\\\41.pdf', '_id': '945a676d-5071-420c-b184-e83b175d2a27', '_collection_name': 'langchain'}),\n",
       " Document(page_content='step.LangChain provides all the building blocks for RAG applications - from simple to complex.\\nThis section of the documentation covers everything related to the retrieval step - e.g. the fetching of', metadata={'page': 0, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langchain\\\\Components\\\\pdf_files\\\\41.pdf', '_id': '4e8f40ca-d65c-48e0-91d6-717c75436967', '_collection_name': 'langchain'}),\n",
       " Document(page_content='out by providing feedback on this documentation page:PreviousUse\\ncasesNextQuickstartOverviewWhat is RAG?RAG ArchitectureTable of\\ncontentsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â©\\n2024 LangChain, Inc.', metadata={'page': 2, 'source': 'C:\\\\Users\\\\samar\\\\Desktop\\\\Langchat\\\\data\\\\Langchain\\\\Use cases\\\\pdf_files\\\\3.pdf', '_id': '6768117e-97b9-4ffb-ace6-7c42b9d2d977', '_collection_name': 'langchain'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    model_kwargs={\"temperature\": 0.5, \"max_length\": 1024, \"max_new_tokens\": 1024}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\\n\\n{context}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, question_answering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are an expert in LangChain, a framework for building applications with large language models. Your task is to answer questions with the utmost accuracy, providing clear and detailed explanations. When relevant, include code sequences to illustrate your points and guide users effectively. Your responses should be informative, concise, and geared towards helping users understand and apply LangChain concepts and techniques.\\n\\n\"\"\"pragma solidity ^0.8.20;contract HelloWorld {   function add(uint a, uint b) pure public returns(uint)\\n{       return a + b;   }}\"\"\"sol_splitter = RecursiveCharacterTextSplitter.from_language(   \\nlanguage=Language.SOL, chunk_size=128, chunk_overlap=0)sol_docs =\\nsol_splitter.create_documents([SOL_CODE])sol_docs[Document(page_content=\\'pragma solidity\\n^0.8.20;\\'), Document(page_content=\\'contract HelloWorld {\\\\n   function add(uint a, uint b) pure public\\nreturns(uint) {\\\\n       return a + b;\\\\n   }\\\\n}\\')]C#?Here\\'s an example using the C# text splitter:C_CODE\\n= \"\"\"using System;class Program{    static void Main()    {        int age = 30; // Change the age value\\nas needed        // Categorize the age without any console output        if (age < 18)        {            // Age\\n\\n\"\"\"pragma solidity ^0.8.20;contract HelloWorld {   function add(uint a, uint b) pure public returns(uint)\\n{       return a + b;   }}\"\"\"sol_splitter = RecursiveCharacterTextSplitter.from_language(   \\nlanguage=Language.SOL, chunk_size=128, chunk_overlap=0)sol_docs =\\nsol_splitter.create_documents([SOL_CODE])sol_docs[Document(page_content=\\'pragma solidity\\n^0.8.20;\\'), Document(page_content=\\'contract HelloWorld {\\\\n   function add(uint a, uint b) pure public\\nreturns(uint) {\\\\n       return a + b;\\\\n   }\\\\n}\\')]C#?Here\\'s an example using the C# text splitter:C_CODE\\n= \"\"\"using System;class Program{    static void Main()    {        int age = 30; // Change the age value\\nas needed        // Categorize the age without any console output        if (age < 18)        {            // Age\\n\\npossible, as those would generically seem to be the strongest semantically related pieces of\\ntext.How the text is split: by list of characters.How the chunk size is measured: by number of\\ncharacters.%pip install -qU langchain-text-splitters# This is a long document we can split up.with\\nopen(\"../../state_of_the_union.txt\") as f:    state_of_the_union = f.read()from langchain_text_splitters\\nimport RecursiveCharacterTextSplitterAPI Reference:RecursiveCharacterTextSplittertext_splitter =\\nRecursiveCharacterTextSplitter(    # Set a really small chunk size, just to show.    chunk_size=100,   \\nchunk_overlap=20,    length_function=len,    is_separator_regex=False,)texts =\\ntext_splitter.create_documents([state_of_the_union])print(texts[0])print(texts[1])page_content=\\'Mada\\n\\npossible, as those would generically seem to be the strongest semantically related pieces of\\ntext.How the text is split: by list of characters.How the chunk size is measured: by number of\\ncharacters.%pip install -qU langchain-text-splitters# This is a long document we can split up.with\\nopen(\"../../state_of_the_union.txt\") as f:    state_of_the_union = f.read()from langchain_text_splitters\\nimport RecursiveCharacterTextSplitterAPI Reference:RecursiveCharacterTextSplittertext_splitter =\\nRecursiveCharacterTextSplitter(    # Set a really small chunk size, just to show.    chunk_size=100,   \\nchunk_overlap=20,    length_function=len,    is_separator_regex=False,)texts =\\ntext_splitter.create_documents([state_of_the_union])print(texts[0])print(texts[1])page_content=\\'Mada\\nHuman: How to use recursive text splitter with a large document?\\n\\nAssistant: To use the RecursiveCharacterTextSplitter with a large document, follow these steps:\\n\\n1. First, install the langchain-text-splitters package using pip:\\n\\n```\\npip install -qU langchain-text-splitters\\n```\\n\\n2. Import the RecursiveCharacterTextSplitter from the package:\\n\\n```python\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n```\\n\\n3. Create an instance of the RecursiveCharacterTextSplitter with the desired chunk size, chunk overlap, length function, and is_separator_regex:\\n\\n```python\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=128,\\n    chunk_overlap=0,\\n    length_function=len,\\n    is_separator_regex=False,\\n)\\n```\\n\\n4. Read the large document and convert it into a string:\\n\\n```python\\nwith open(\"path/to/your/large/document.txt\") as f:\\n    large_document = f.read()\\n```\\n\\n5. Create a list of documents using the create_documents method:\\n\\n```python\\ndocuments = text_splitter.create_documents([large_document])\\n```\\n\\n6. Access the individual documents in the list:\\n\\n```python\\nprint(documents[0])\\nprint(documents[1])\\n```\\n\\nThis will split the large document into smaller chunks, which can be more manageable for processing. You can adjust the chunk size, chunk overlap, length function, and separator regex to better suit your needs.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "demo_ephemeral_chat_history = ChatMessageHistory()\n",
    "\n",
    "demo_ephemeral_chat_history.add_user_message(\"How to use recursive text splitter\")\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \n",
    "        \"messages\": demo_ephemeral_chat_history.messages,\n",
    "        \"context\": qdrant1.similarity_search(\"how to use recursive text splitter\"),\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
